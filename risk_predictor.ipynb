{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7abd62fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ S3 Connection successful!\n",
      "Files in your bucket:\n",
      "  📄 Unsaved/2025/08/01/3384ee98-9d89-4764-91bc-7b734c13eb37.csv (2010 bytes)\n",
      "  📄 Unsaved/2025/08/01/3384ee98-9d89-4764-91bc-7b734c13eb37.csv.metadata (702 bytes)\n",
      "  📄 Unsaved/2025/08/01/6618b27b-5c4f-4644-9130-5d3c1561c39b.csv (2010 bytes)\n",
      "  📄 Unsaved/2025/08/01/6618b27b-5c4f-4644-9130-5d3c1561c39b.csv.metadata (702 bytes)\n",
      "  📄 processed/ (0 bytes)\n",
      "  📄 raw/ (0 bytes)\n",
      "  📄 raw/bus_saftey.csv (3390536 bytes)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Test AWS connection\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List your bucket contents\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket='transperth-data-shandilya')\n",
    "    print(\"✅ S3 Connection successful!\")\n",
    "    \n",
    "    if 'Contents' in response:\n",
    "        print(\"Files in your bucket:\")\n",
    "        for obj in response['Contents']:\n",
    "            print(f\"  📄 {obj['Key']} ({obj['Size']} bytes)\")\n",
    "    else:\n",
    "        print(\"Bucket is empty\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4b37e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2️⃣ Loading CSV from S3...\n",
      "✅ Successfully loaded 23158 rows from S3\n",
      "📊 Columns: ['Year', 'Date Of Incident', 'Route', 'Operator', 'Group Name', 'Bus Garage', 'Borough', 'Injury Result Description', 'Incident Event Type', 'Victim Category', 'Victims Sex', 'Victims Age']\n",
      "\n",
      "3️⃣ Cleaning data...\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Load CSV from S3\n",
    "print(\"\\n2️⃣ Loading CSV from S3...\")\n",
    "try:\n",
    "    # Get the CSV file\n",
    "    obj = s3.get_object(Bucket=BUCKET_NAME, Key=FILE_KEY)\n",
    "    csv_content = obj['Body'].read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(csv_content))\n",
    "    \n",
    "    print(f\"✅ Successfully loaded {len(df)} rows from S3\")\n",
    "    print(f\"📊 Columns: {list(df.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading CSV: {e}\")\n",
    "    print(\"Make sure bus_saftey.csv is uploaded to s3://transperth-data-shandilya/raw/\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: Data Cleaning\n",
    "print(\"\\n3️⃣ Cleaning data...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "622761a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3️⃣ Cleaning data...\n",
      "🧹 Cleaned column names: ['year', 'date_of_incident', 'route', 'operator', 'group_name', 'bus_garage', 'borough', 'injury_result_description', 'incident_event_type', 'victim_category', 'victims_sex', 'victims_age']\n",
      "📈 Rows after cleaning: 23158 (removed 0 rows with missing data)\n",
      "\n",
      "4️⃣ Creating target variable...\n",
      "🎯 High risk cases: 5822 out of 23158 (25.14%)\n",
      "\n",
      "📋 Sample injury descriptions:\n",
      "Injuries treated on scene                                          17336\n",
      "Taken to Hospital ? Reported Serious Injury or Severity Unknown     2994\n",
      "Reported Minor Injury - Treated at Hospital                         2786\n",
      "Fatal                                                                 42\n",
      "Name: injury_result_description, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Data Cleaning\n",
    "print(\"\\n3️⃣ Cleaning data...\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "print(f\"🧹 Cleaned column names: {list(df.columns)}\")\n",
    "\n",
    "# Define columns we want to use\n",
    "desired_cols = [\n",
    "    'route', \n",
    "    'borough', \n",
    "    'injury_result_description', \n",
    "    'incident_event_type', \n",
    "    'victim_category', \n",
    "    'victims_sex', \n",
    "    'victims_age'\n",
    "]\n",
    "\n",
    "# Check which columns actually exist\n",
    "available_cols = [col for col in desired_cols if col in df.columns]\n",
    "missing_cols = [col for col in desired_cols if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"⚠️  Missing columns: {missing_cols}\")\n",
    "    print(f\"✅ Using available columns: {available_cols}\")\n",
    "\n",
    "if not available_cols:\n",
    "    print(\"❌ No required columns found! Please check your CSV structure.\")\n",
    "    exit()\n",
    "\n",
    "# Use available columns and remove missing data\n",
    "df_clean = df[available_cols].dropna()\n",
    "print(f\"📈 Rows after cleaning: {len(df_clean)} (removed {len(df) - len(df_clean)} rows with missing data)\")\n",
    "\n",
    "# Step 4: Create Target Variable\n",
    "print(\"\\n4️⃣ Creating target variable...\")\n",
    "\n",
    "if 'injury_result_description' in df_clean.columns:\n",
    "    # Create high_risk target based on injury severity\n",
    "    df_clean['high_risk'] = df_clean['injury_result_description'].apply(\n",
    "        lambda x: 1 if any(keyword in str(x).lower() for keyword in ['serious', 'hospital', 'fatal', 'severe']) else 0\n",
    "    )\n",
    "    \n",
    "    high_risk_count = df_clean['high_risk'].sum()\n",
    "    total_count = len(df_clean)\n",
    "    \n",
    "    print(f\"🎯 High risk cases: {high_risk_count} out of {total_count} ({high_risk_count/total_count:.2%})\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\n📋 Sample injury descriptions:\")\n",
    "    print(df_clean['injury_result_description'].value_counts().head())\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No 'injury_result_description' column found - cannot create target variable\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d65280ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5️⃣ Preparing features...\n",
      "🏷️  Categorical columns: ['route', 'borough', 'incident_event_type', 'victim_category', 'victims_sex', 'victims_age']\n",
      "🔢 Final feature count: 682 columns\n",
      "📊 Dataset shape: 23158 rows × 682 features\n",
      "\n",
      "6️⃣ Training machine learning model...\n",
      "⚖️  Class distribution: {0: 17336, 1: 5822}\n",
      "⚖️  Applied class weights: {0: 0.6679164743885556, 1: 1.988835451734799}\n",
      "✅ Model training completed!\n",
      "\n",
      "7️⃣ Analyzing feature importance...\n",
      "🏆 Top 10 Risk Factors:\n",
      " 1. victims_age_Elderly: 0.1301\n",
      " 2. incident_event_type_Collision Incident: 0.1245\n",
      " 3. incident_event_type_Personal Injury: 0.1046\n",
      " 4. victims_age_Unknown: 0.0883\n",
      " 5. victim_category_Pedestrian: 0.0757\n",
      " 6. victim_category_Passenger: 0.0425\n",
      " 7. victims_sex_Unknown: 0.0327\n",
      " 8. victims_age_Adult: 0.0234\n",
      " 9. incident_event_type_Onboard Injuries: 0.0175\n",
      "10. victims_age_Child: 0.0151\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Feature Engineering\n",
    "print(\"\\n5️⃣ Preparing features...\")\n",
    "\n",
    "# Get categorical columns (exclude target and description)\n",
    "categorical_cols = [col for col in available_cols if col not in ['injury_result_description']]\n",
    "print(f\"🏷️  Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, prefix=categorical_cols)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_encoded.drop(['injury_result_description', 'high_risk'], axis=1)\n",
    "y = df_encoded['high_risk']\n",
    "\n",
    "print(f\"🔢 Final feature count: {X.shape[1]} columns\")\n",
    "print(f\"📊 Dataset shape: {X.shape[0]} rows × {X.shape[1]} features\")\n",
    "\n",
    "# Step 6: Train Model\n",
    "print(\"\\n6️⃣ Training machine learning model...\")\n",
    "\n",
    "if y.sum() == 0:\n",
    "    print(\"❌ No high-risk cases found! Cannot train model.\")\n",
    "    print(\"💡 Try adjusting the high-risk criteria in step 4\")\n",
    "    exit()\n",
    "\n",
    "# Handle class imbalance with class weights\n",
    "classes = np.unique(y)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "print(f\"⚖️  Class distribution: {dict(y.value_counts())}\")\n",
    "print(f\"⚖️  Applied class weights: {class_weight_dict}\")\n",
    "\n",
    "# Train Random Forest with balanced weights\n",
    "model = RandomForestClassifier(\n",
    "    random_state=42, \n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    max_depth=10\n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "print(\"✅ Model training completed!\")\n",
    "\n",
    "# Step 7: Feature Importance Analysis\n",
    "print(\"\\n7️⃣ Analyzing feature importance...\")\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.Series(model.feature_importances_, index=X.columns)\n",
    "top_features = feature_importance.nlargest(15)\n",
    "\n",
    "print(\"🏆 Top 10 Risk Factors:\")\n",
    "for i, (feature, importance) in enumerate(top_features.head(10).items(), 1):\n",
    "    print(f\"{i:2d}. {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a819dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9️⃣ Model Summary:\n",
      "========================================\n",
      "📈 Training Accuracy: 0.696\n",
      "🌳 Number of Trees: 100\n",
      "🔍 Features Used: 682\n",
      "\n",
      "🎯 Feature Importance Stats:\n",
      "   • Most important: victims_age_Elderly (0.1301)\n",
      "   • Average importance: 0.0015\n",
      "   • Features >1% importance: 14\n",
      "\n",
      "📊 Dataset Summary:\n",
      "   • Total incidents: 23,158\n",
      "   • High-risk incidents: 5,822 (25.14%)\n",
      "   • Low-risk incidents: 17,336 (74.86%)\n",
      "\n",
      "🎉 Analysis completed successfully!\n",
      "==================================================\n",
      "💾 Feature importance saved to 'feature_importance_results.csv'\n",
      "\n",
      "💡 Next steps:\n",
      "1. Review the top risk factors in the chart\n",
      "2. Focus safety initiatives on high-importance features\n",
      "3. Collect more data for better model accuracy\n",
      "4. Consider additional features like weather, time of day, etc.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 9: Model Performance Summary\n",
    "print(\"\\n9️⃣ Model Summary:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Basic model info\n",
    "train_accuracy = model.score(X, y)\n",
    "print(f\"📈 Training Accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"🌳 Number of Trees: {model.n_estimators}\")\n",
    "print(f\"🔍 Features Used: {len(X.columns)}\")\n",
    "\n",
    "# Feature importance stats\n",
    "print(f\"\\n🎯 Feature Importance Stats:\")\n",
    "print(f\"   • Most important: {top_features.index[0]} ({top_features.iloc[0]:.4f})\")\n",
    "print(f\"   • Average importance: {feature_importance.mean():.4f}\")\n",
    "print(f\"   • Features >1% importance: {(feature_importance > 0.01).sum()}\")\n",
    "\n",
    "# Data summary\n",
    "print(f\"\\n📊 Dataset Summary:\")\n",
    "print(f\"   • Total incidents: {len(df_clean):,}\")\n",
    "print(f\"   • High-risk incidents: {y.sum():,} ({y.mean():.2%})\")\n",
    "print(f\"   • Low-risk incidents: {(y == 0).sum():,} ({(y == 0).mean():.2%})\")\n",
    "\n",
    "print(\"\\n🎉 Analysis completed successfully!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Optional: Save results to CSV\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': top_features.index,\n",
    "    'Importance': top_features.values\n",
    "})\n",
    "\n",
    "feature_importance_df.to_csv('feature_importance_results.csv', index=False)\n",
    "print(\"💾 Feature importance saved to 'feature_importance_results.csv'\")\n",
    "\n",
    "print(\"\\n💡 Next steps:\")\n",
    "print(\"1. Review the top risk factors in the chart\")\n",
    "print(\"2. Focus safety initiatives on high-importance features\")\n",
    "print(\"3. Collect more data for better model accuracy\")\n",
    "print(\"4. Consider additional features like weather, time of day, etc.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306707e1",
   "metadata": {},
   "source": [
    "# code for getting tableau ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41f81911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Preparing data for Tableau Dashboard...\n",
      "==================================================\n",
      "📊 Loaded 23158 records\n",
      "📅 Found date columns: ['date_of_incident']\n",
      "✅ Tableau-ready data saved to 'bus_safety_tableau_ready.csv'\n",
      "✅ Data also saved to S3: s3://transperth-data-shandilya/processed/\n",
      "\n",
      "📈 Dashboard Data Summary:\n",
      "==============================\n",
      "Total Incidents: 23,158\n",
      "High Risk Incidents: 5,822\n",
      "Risk Rate: 25.1%\n",
      "\n",
      "👥 Age Groups:\n",
      "   Young Adult (18-35): 10754\n",
      "   Unknown Age: 7454\n",
      "   Senior (55+): 2769\n",
      "   Child (0-12): 2181\n",
      "\n",
      "🏘️  Top Risk Areas:\n",
      "   Kensington & Chelsea: 32.0%\n",
      "   Westminster: 29.8%\n",
      "   None London Borough: 28.7%\n",
      "   City of London: 28.4%\n",
      "   Sutton: 28.0%\n",
      "\n",
      "🚨 Incident Types:\n",
      "   Other: 12011\n",
      "   Slip/Fall: 6981\n",
      "   Vehicle Collision: 4166\n",
      "\n",
      "📊 Columns created for Tableau: 21\n",
      "Key columns: ['injury_severity', 'high_risk', 'age_group', 'risk_score', 'risk_level', 'incident_category', 'total_incidents', 'risk_percentage']\n",
      "\n",
      "🎯 Ready for Tableau! Use the 'bus_safety_tableau_ready.csv' file\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# Load your bus safety data from S3\n",
    "BUCKET_NAME = 'transperth-data-shandilya'\n",
    "FILE_KEY = 'raw/bus_saftey.csv'\n",
    "\n",
    "print(\"🚀 Preparing data for Tableau Dashboard...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load data from S3\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=BUCKET_NAME, Key=FILE_KEY)\n",
    "csv_content = obj['Body'].read().decode('utf-8')\n",
    "df = pd.read_csv(StringIO(csv_content))\n",
    "\n",
    "print(f\"📊 Loaded {len(df)} records\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Create enhanced dataset for Tableau\n",
    "tableau_data = df.copy()\n",
    "\n",
    "# 1. Create Risk Categories\n",
    "def categorize_injury_severity(injury_desc):\n",
    "    injury_str = str(injury_desc).lower()\n",
    "    if any(word in injury_str for word in ['fatal', 'death', 'died']):\n",
    "        return 'Fatal'\n",
    "    elif any(word in injury_str for word in ['serious', 'severe', 'critical']):\n",
    "        return 'Serious'\n",
    "    elif any(word in injury_str for word in ['hospital', 'admitted', 'emergency']):\n",
    "        return 'Hospital Required'\n",
    "    elif any(word in injury_str for word in ['minor', 'slight', 'bruise', 'scratch']):\n",
    "        return 'Minor'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "if 'injury_result_description' in tableau_data.columns:\n",
    "    tableau_data['injury_severity'] = tableau_data['injury_result_description'].apply(categorize_injury_severity)\n",
    "    tableau_data['high_risk'] = tableau_data['injury_severity'].apply(\n",
    "        lambda x: 'High Risk' if x in ['Fatal', 'Serious', 'Hospital Required'] else 'Low Risk'\n",
    "    )\n",
    "\n",
    "# 2. Create Age Groups\n",
    "def categorize_age(age):\n",
    "    age_str = str(age).lower()\n",
    "    if any(word in age_str for word in ['child', 'infant', 'toddler', 'kid']):\n",
    "        return 'Child (0-12)'\n",
    "    elif any(word in age_str for word in ['teen', 'adolescent', '13', '14', '15', '16', '17']):\n",
    "        return 'Teen (13-17)' \n",
    "    elif any(word in age_str for word in ['young', 'adult', '18', '19', '20', '30']):\n",
    "        return 'Young Adult (18-35)'\n",
    "    elif any(word in age_str for word in ['middle', 'adult', '40', '50']):\n",
    "        return 'Middle Age (36-55)'\n",
    "    elif any(word in age_str for word in ['senior', 'elderly', 'old', '60', '70', '80']):\n",
    "        return 'Senior (55+)'\n",
    "    else:\n",
    "        return 'Unknown Age'\n",
    "\n",
    "if 'victims_age' in tableau_data.columns:\n",
    "    tableau_data['age_group'] = tableau_data['victims_age'].apply(categorize_age)\n",
    "\n",
    "# 3. Create Time Categories (if date/time columns exist)\n",
    "# Add day of week, month, hour analysis if timestamp columns are available\n",
    "date_columns = [col for col in tableau_data.columns if any(word in col.lower() for word in ['date', 'time', 'when'])]\n",
    "if date_columns:\n",
    "    print(f\"📅 Found date columns: {date_columns}\")\n",
    "    # You can add time-based analysis here based on your actual date columns\n",
    "\n",
    "# 4. Create Location Risk Scores\n",
    "if 'borough' in tableau_data.columns:\n",
    "    # Calculate risk score by borough\n",
    "    borough_risk = tableau_data.groupby('borough').agg({\n",
    "        'high_risk': lambda x: (x == 'High Risk').mean(),\n",
    "        'injury_result_description': 'count'\n",
    "    }).reset_index()\n",
    "    borough_risk.columns = ['borough', 'risk_score', 'total_incidents']\n",
    "    borough_risk['risk_level'] = pd.cut(borough_risk['risk_score'], \n",
    "                                       bins=[0, 0.2, 0.4, 0.6, 1.0], \n",
    "                                       labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    # Merge back to main data\n",
    "    tableau_data = tableau_data.merge(borough_risk[['borough', 'risk_score', 'risk_level']], \n",
    "                                     on='borough', how='left')\n",
    "\n",
    "# 5. Create Incident Type Categories\n",
    "if 'incident_event_type' in tableau_data.columns:\n",
    "    def categorize_incident_type(incident):\n",
    "        incident_str = str(incident).lower()\n",
    "        if any(word in incident_str for word in ['collision', 'crash', 'impact']):\n",
    "            return 'Vehicle Collision'\n",
    "        elif any(word in incident_str for word in ['slip', 'fall', 'trip']):\n",
    "            return 'Slip/Fall'\n",
    "        elif any(word in incident_str for word in ['door', 'boarding', 'alighting']):\n",
    "            return 'Boarding/Alighting'\n",
    "        elif any(word in incident_str for word in ['sudden', 'brake', 'jerk', 'stop']):\n",
    "            return 'Sudden Movement'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    tableau_data['incident_category'] = tableau_data['incident_event_type'].apply(categorize_incident_type)\n",
    "\n",
    "# 6. Create KPI Columns for Dashboard\n",
    "total_incidents = len(tableau_data)\n",
    "if 'high_risk' in tableau_data.columns:\n",
    "    high_risk_incidents = (tableau_data['high_risk'] == 'High Risk').sum()\n",
    "    tableau_data['total_incidents'] = total_incidents\n",
    "    tableau_data['high_risk_count'] = high_risk_incidents\n",
    "    tableau_data['risk_percentage'] = (high_risk_incidents / total_incidents) * 100\n",
    "\n",
    "# 7. Save prepared data for Tableau\n",
    "# Save to local CSV\n",
    "output_file = 'bus_safety_tableau_ready.csv'\n",
    "tableau_data.to_csv(output_file, index=False)\n",
    "print(f\"✅ Tableau-ready data saved to '{output_file}'\")\n",
    "\n",
    "# Also save to S3 processed folder\n",
    "try:\n",
    "    s3.put_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key='processed/bus_safety_tableau_ready.csv',\n",
    "        Body=tableau_data.to_csv(index=False)\n",
    "    )\n",
    "    print(\"✅ Data also saved to S3: s3://transperth-data-shandilya/processed/\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not save to S3: {e}\")\n",
    "\n",
    "# 8. Create Summary Statistics for Dashboard\n",
    "print(\"\\n📈 Dashboard Data Summary:\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Total Incidents: {total_incidents:,}\")\n",
    "\n",
    "if 'high_risk' in tableau_data.columns:\n",
    "    print(f\"High Risk Incidents: {high_risk_incidents:,}\")\n",
    "    print(f\"Risk Rate: {(high_risk_incidents/total_incidents)*100:.1f}%\")\n",
    "\n",
    "if 'age_group' in tableau_data.columns:\n",
    "    print(f\"\\n👥 Age Groups:\")\n",
    "    for age, count in tableau_data['age_group'].value_counts().head().items():\n",
    "        print(f\"   {age}: {count}\")\n",
    "\n",
    "if 'borough' in tableau_data.columns:\n",
    "    print(f\"\\n🏘️  Top Risk Areas:\")\n",
    "    risk_areas = tableau_data.groupby('borough')['high_risk'].apply(lambda x: (x == 'High Risk').mean()).sort_values(ascending=False)\n",
    "    for area, risk in risk_areas.head().items():\n",
    "        print(f\"   {area}: {risk:.1%}\")\n",
    "\n",
    "if 'incident_category' in tableau_data.columns:\n",
    "    print(f\"\\n🚨 Incident Types:\")\n",
    "    for incident, count in tableau_data['incident_category'].value_counts().head().items():\n",
    "        print(f\"   {incident}: {count}\")\n",
    "\n",
    "print(f\"\\n📊 Columns created for Tableau: {len(tableau_data.columns)}\")\n",
    "print(\"Key columns:\", [col for col in tableau_data.columns if col in [\n",
    "    'injury_severity', 'high_risk', 'age_group', 'risk_score', 'risk_level', \n",
    "    'incident_category', 'total_incidents', 'risk_percentage'\n",
    "]])\n",
    "\n",
    "print(\"\\n🎯 Ready for Tableau! Use the 'bus_safety_tableau_ready.csv' file\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe934ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
